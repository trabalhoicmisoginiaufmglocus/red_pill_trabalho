{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673e69ea-581e-40d5-b497-0b6a8617c77e",
   "metadata": {
    "id": "673e69ea-581e-40d5-b497-0b6a8617c77e"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a27765e-167c-4089-9daf-401780f3bc55",
   "metadata": {
    "id": "3a27765e-167c-4089-9daf-401780f3bc55"
   },
   "outputs": [],
   "source": [
    "# Separe aqui seus dados pra fazer a inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5e72e-2e37-436f-8169-f3d603684203",
   "metadata": {
    "id": "e5d5e72e-2e37-436f-8169-f3d603684203"
   },
   "source": [
    "# Setup Llama 3.1:8b Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16fe8a7c-8fd6-4bc7-8aa9-409756fdf2b5",
   "metadata": {
    "id": "16fe8a7c-8fd6-4bc7-8aa9-409756fdf2b5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7acdd5c-ade8-4253-b5e3-086d4c3c9a91",
   "metadata": {
    "id": "d7acdd5c-ade8-4253-b5e3-086d4c3c9a91"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='llm_inferences.log',\n",
    "    filemode='a'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7264893b-8d1b-40cf-a217-286f73c29d49",
   "metadata": {
    "id": "7264893b-8d1b-40cf-a217-286f73c29d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from /home/jovyan/datalake/models/llama3-8b...\n",
      "pad_token defined as eos_token for tokenizer.\n",
      "Loading model from /home/jovyan/datalake/models/llama3-8b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ea3be9fc274cf79e0192ddc4f0b5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_path = \"/home/jovyan/datalake/models/llama3-8b\"\n",
    "\n",
    "print(f\"Loading tokenizer from {local_path}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    local_path,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"pad_token defined as eos_token for tokenizer.\")\n",
    "\n",
    "print(f\"Loading model from {local_path}...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_path,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2626f56-2ac9-47be-948e-c885b7c37131",
   "metadata": {
    "id": "c2626f56-2ac9-47be-948e-c885b7c37131"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "702294ef-90ed-4ab5-9bac-bba98793f243",
   "metadata": {
    "id": "702294ef-90ed-4ab5-9bac-bba98793f243"
   },
   "outputs": [],
   "source": [
    "generation_params = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"do_sample\": False,\n",
    "    #\"temperature\": 0.7,\n",
    "    \"truncation\": False,\n",
    "    \"return_full_text\": False,\n",
    "}\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410239a6-5ec1-4639-9549-53399ce2db29",
   "metadata": {
    "id": "410239a6-5ec1-4639-9549-53399ce2db29"
   },
   "outputs": [],
   "source": [
    "def save_batch_to_csv(batch, results_to_save):\n",
    "    \"\"\"\n",
    "    Save a batch result into a CSV file, creating the header if the file\n",
    "    does not exist or is empty.\n",
    "\n",
    "    Args:\n",
    "        batch (int): Batch ID\n",
    "        results_to_save (list): List of llm results obtained (prompt + output)\n",
    "    \"\"\"\n",
    "\n",
    "    output_csv_file = f\"llm_inferences/llama3.1-8b-instruct/{batch}.csv\"\n",
    "    fieldnames = [\"original_id\", \"original_prompt\", \"llm_response\"]\n",
    "\n",
    "    output_dir = Path(output_csv_file).parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    write_header = not os.path.exists(output_csv_file) or os.path.getsize(output_csv_file) == 0\n",
    "\n",
    "    try:\n",
    "        with open(output_csv_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "                logging.info(f\"Header written to '{output_csv_file}'.\")\n",
    "\n",
    "            writer.writerows(results_to_save)\n",
    "\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Error on saving batch to CSV '{output_csv_file}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b76fe421-7188-4515-91cc-b7f9ca0af43a",
   "metadata": {
    "id": "b76fe421-7188-4515-91cc-b7f9ca0af43a"
   },
   "outputs": [],
   "source": [
    "PROGRESS_FILE = 'inferences.json'\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load the progress from a JSON file.\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            logging.info(f\"Progress file '{PROGRESS_FILE}' found. Loading state.\")\n",
    "            return json.load(f)\n",
    "    logging.info(f\"No progress file '{PROGRESS_FILE}' found. Starting from zero.\")\n",
    "    return {}\n",
    "\n",
    "def save_progress(progress_data):\n",
    "    \"\"\"Save the current progress into a JSON file\"\"\"\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump(progress_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7e7108-f1fb-445d-89eb-a022110ec9b6",
   "metadata": {
    "id": "bc7e7108-f1fb-445d-89eb-a022110ec9b6"
   },
   "outputs": [],
   "source": [
    "progress = load_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc3e026-8d1e-43a6-9642-6122bcb583ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEFINIÇÃO DE TODOS OS TESTES ---\n",
    "\n",
    "all_tests = [\n",
    "    # --- Teste 1: Zero-Shot, Português, Sem Conceito ---\n",
    "    {\n",
    "        \"name\": \"teste1_pt_sem_conceito\",\n",
    "        \"system_prompt_base\": (\n",
    "            \"Você é um classificador de texto altamente preciso de títulos e descrições do Youtube. \"\n",
    "            \"Sua única função é analisar o texto e responder estritamente com '0' ou '1'.\\n\"\n",
    "            \"A pergunta que você deve responder é: O texto é {task_definition}?\"\n",
    "            \"1: O texto É {task_definition}\\n\"\n",
    "            \"0: O texto NÃO é {task_definition}\\n\"\n",
    "            \"Não forneça nenhuma explicação, apenas o número.\"\n",
    "        ),\n",
    "        \"task_definitions\": {\n",
    "            \"toxicidade\": \"TÓXICO\",\n",
    "            \"misoginia\": \"MISÓGINO\"\n",
    "        },\n",
    "        \"few_shot_config\": {\n",
    "            \"toxicidade\": {\"examples\": [], \"output_prefix\": \"results_toxic\"},\n",
    "            \"misoginia\": {\"examples\": [], \"output_prefix\": \"results_mysogyny\"}\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # --- Teste 2: Zero-Shot, Inglês, Sem Conceito ---\n",
    "    {\n",
    "        \"name\": \"teste2_en_sem_conceito\",\n",
    "        \"system_prompt_base\": (\n",
    "            \"You are a highly accurate text classifier for Youtube titles and descriptions. \"\n",
    "            \"Your sole function is to analyze the text and respond strictly with '0' or '1'.\\n\"\n",
    "            \"The question you must answer is: Is the text {task_definition}?\\n\"\n",
    "            \"1: The text IS {task_definition}\\n\"\n",
    "            \"0: The text is NOT {task_definition}\\n\"\n",
    "            \"Do not provide any explanation, just the number.\"\n",
    "        ),\n",
    "        \"task_definitions\": {\n",
    "            \"toxicidade\": \"TOXIC\",\n",
    "            \"misoginia\": \"MISOGYNISTIC\"\n",
    "        },\n",
    "        \"few_shot_config\": {\n",
    "            \"toxicidade\": {\"examples\": [], \"output_prefix\": \"results_toxic\"},\n",
    "            \"misoginia\": {\"examples\": [], \"output_prefix\": \"results_mysogyny\"}\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # --- Teste 3: Zero-Shot, Português, Com Conceito ---\n",
    "    {\n",
    "        \"name\": \"teste3_pt_com_conceito\",\n",
    "        \"system_prompt_base\": (\n",
    "            \"Você é um classificador de texto altamente preciso de títulos e descrições do Youtube. \"\n",
    "            \"Sua única função é analisar o texto e responder estritamente com '0' ou '1'.\\n\"\n",
    "            \"Você deve analisar se o texto contém {task_definition}. Assim, responda:\"\n",
    "            \"1: Se o texto se encaixa em qualquer uma das categorias de conceito listadas \\n\"\n",
    "            \"0: Se o texto NÃO se encaixa em qualquer uma das categorias de conceito listadas\\n\"\n",
    "            \"Não forneça nenhuma explicação, apenas o número.\"\n",
    "        ),\n",
    "        \"task_definitions\": {\n",
    "            \"toxicidade\": \"TOXICIDADE - Utilize as seguintes definições para guiar sua classificação: \\n1. Ódio e Assédio:\\n1.1 Doxing: Divulgação de informações pessoais de um alvo para o público em geral, com o objetivo de prejudicá-lo.\\n1.2 Ataque à identidade: Consiste em ataques severos a indivíduos ou grupos com base em sua afiliação a um grupo protegido ou vulnerável.\\n1.3 Falsa representação de identidade: É a disseminação de declarações que comunicam deturpações pejorativas, estereótipos e outras generalizações insultuosas sobre populações protegidas ou vulneráveis.\\n1.4 Insulto: Ataque pessoal direcionado, como xingamentos, comentários negativos, depreciativos e inflamatórios sobre o indivíduo.\\n1.5 Agressão Sexual: Inclui avanços sexuais indesejados, sexualização indesejável e compartilhamento não consensual de conteúdo sexual, entre outras formas de conversas sexuais não solicitadas.\\n1.6 Ameaça de Violência: Refere-se a conteúdo que expressa o desejo de matar ou infligir dano físico, celebra atos violentos ou incita outros a cometerem violência.\\n2. Dano autoinfligido:\\n2.1 Promoção de transtorno alimentar: Abrange conteúdo que promove transtornos alimentares como escolhas de estilo de vida legítimas ou glorifica corpos excessivamente magros.\\n2.2 Autolesão: Ideação de se machucar fisicamente de propósito para lidar com o sofrimento emocional, incluindo também a ideação suicida.\\n3. Dano ideológico:\\n3.1 Extremismo, terrorismo e crime organizado: Inclui conteúdo relacionado ao recrutamento, elogio ou assistência a grupos extremistas, terroristas ou de crime organizado.\\n3.2 Desinformação: É a informação falsa ou enganosa, que pode ser disseminada com ou sem a intenção deliberada de prejudicar.\\n4. Exploração:\\n4.1 Serviços sexuais adultos: Refere-se à promoção ou solicitação de serviços sexuais ilegais que exploram participantes frequentemente vulneráveis.\\n4.2 Material de abuso sexual infantil: É definido como conteúdo que envolve o abuso e a exploração sexual de qualquer pessoa menor de dezoito anos.\\n4.3 Golpes: São tentativas de enganar uma pessoa para que ela forneça fundos ou informações sensíveis usando técnicas fraudulentas ou invasivas.\\n\",\n",
    "            \"misoginia\": \"MISOGINIA - Utilize as seguintes definições para guiar sua classificação: \\n1. Ameaças, planos para prejudicar e incitamento:\\n1.1 Ameaças de danos: Expressar desejo de prejudicar física, sexual ou emocionalmente uma mulher ou grupo de mulheres.\\n1.2 Incitamento e incentivo ao dano: Encorajar ou justificar que outros prejudiquem uma mulher ou grupo de mulheres.\\n2. Depreciação:\\n2.1 Ataques descritivos: Fazer generalizações negativas sobre as habilidades, aparência, intelecto ou caráter das mulheres.\\n2.2 Ataques agressivos e emotivos: Expressar ódio ou nojo contra as mulheres, usando insultos de gênero ou acusações infundadas.\\n2.3 Ataques desumanizantes e objetificação sexual explícita: Comparar mulheres a animais/objetos ou reduzi-las a objetos sexuais.\\n3. Animosidade:\\n3.1 Uso casual de gírias de gênero, profanidades e insultos: Usar termos pejorativos para mulheres de forma casual, não como um ataque direto.\\n3.2 Diferenças imutáveis de gênero e estereótipos de gênero: Afirmar diferenças 'naturais' entre homens e mulheres para implicar inferioridade ou limitar papéis (inclui a maioria das piadas sexistas).\\n3.3 Elogios de gênero ambíguos: Elogiar mulheres de forma que reforce estereótipos de fragilidade, ou que reduza seu valor à sua atratividade.\\n3.4 Explicações condescendentes ou conselhos indesejados: Oferecer conselhos paternalistas a mulheres sobre assuntos que elas dominam ('mansplaining').\\n4. Discussão Preconceituosa:\\n4.1 Apoio ao mau tratamento de mulheres individuais: Justificar, negar ou subestimar o mau tratamento de uma mulher.\\n4.2 Apoio à discriminação sistêmica contra mulheres como grupo: Justificar, negar ou subestimar a discriminação contra as mulheres como um grupo.\\n\"\n",
    "        },\n",
    "        \"few_shot_config\": {\n",
    "            \"toxicidade\": {\"examples\": [], \"output_prefix\": \"results_toxic\"},\n",
    "            \"misoginia\": {\"examples\": [], \"output_prefix\": \"results_mysogyny\"}\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # --- Teste 4: Zero-Shot, Inglês, Com Conceito ---\n",
    "    {\n",
    "        \"name\": \"teste4_en_com_conceito\",\n",
    "        \"system_prompt_base\": (\n",
    "            \"You are a highly accurate text classifier for Youtube titles and descriptions. \"\n",
    "            \"Your sole function is to analyze the text and respond strictly with '0' or '1'.\\n\"\n",
    "            \"You must analyze if the text contains {task_definition}. Accordingly, respond:\\n\"\n",
    "            \"1: If the text fits into any of the listed concept categories\\n\"\n",
    "            \"0: If the text does NOT fit into any of the listed concept categories\\n\"\n",
    "            \"Do not provide any explanation, just the number.\"\n",
    "        ),\n",
    "        \"task_definitions\": {\n",
    "            \"toxicidade\": \"TOXICITY - Use the following definitions to guide your classification: \\n1. Hate and Harassment:\\n1.1 Doxing: Disclosure of a target's personal information to the general public with the intent to harm them.\\n1.2 Identity Attack: Consists of severe attacks on individuals or groups based on their affiliation with a protected or vulnerable group.\\n1.3 Misrepresentation of Identity: Is the dissemination of statements that communicate pejorative misrepresentations, stereotypes, and other insulting generalizations about protected or vulnerable populations.\\n1.4 Insult: A directed personal attack, such as name-calling, negative, derogatory, and inflammatory comments about the individual.\\n1.5 Sexual Aggression: Includes unwanted sexual advances, unwelcome sexualization, and non-consensual sharing of sexual content, among other forms of unsolicited sexual conversations.\\n1.6 Threat of Violence: Refers to content that expresses the desire to kill or inflict physical harm, celebrates violent acts, or incites others to commit violence.\\n2. Self-Harm:\\n2.1 Promotion of Eating Disorders: Covers content that promotes eating disorders as legitimate lifestyle choices or glorifies excessively thin bodies.\\n2.2 Self-Injury: Ideation of purposely physically hurting oneself to cope with emotional distress, including suicidal ideation.\\n3. Ideological Harm:\\n3.1 Extremism, Terrorism, and Organized Crime: Includes content related to the recruitment, praise, or assistance of extremist, terrorist, or organized crime groups.\\n3.2 Disinformation: Is false or misleading information, which can be disseminated with or without the deliberate intention to cause harm.\\n4. Exploitation:\\n4.1 Adult Sexual Services: Refers to the promotion or solicitation of illegal sexual services that exploit often-vulnerable participants.\\n4.2 Child Sexual Abuse Material: Is defined as content that involves the sexual abuse and exploitation of any person under the age of eighteen.\\n4.3 Scams: Are attempts to deceive a person into providing funds or sensitive information using fraudulent or invasive techniques.\\n\",\n",
    "            \"misoginia\": \"MISOGYNY - Use the following definitions to guide your classification: \\n1. Threats, plans to harm, and incitement:\\n1.1 Threats of harm: Expressing a desire to physically, sexually, or emotionally harm a woman or group of women.\\n1.2 Incitement and encouragement of harm: Encouraging or justifying others to harm a woman or group of women.\\n2. Derogation:\\n2.1 Descriptive attacks: Making negative generalizations about women's skills, appearance, intellect, or character.\\n2.2 Aggressive and emotive attacks: Expressing hatred or disgust against women, using gendered slurs or baseless accusations.\\n2.3 Dehumanizing attacks and explicit sexual objectification: Comparing women to animals/objects or reducing them to sexual objects.\\n3. Animosity:\\n3.1 Casual use of gendered slurs, profanity, and insults: Using derogatory terms for women casually, not as a direct attack.\\n3.2 Immutable gender differences and gender stereotypes: Asserting 'natural' differences between men and women to imply inferiority or limit roles (includes most sexist jokes).\\n3.3 Ambiguous gendered praise: Praising women in a way that reinforces stereotypes of fragility, or that reduces their value to their attractiveness.\\n3.4 Condescending explanations or unsolicited advice: Offering paternalistic advice to women on subjects they are knowledgeable about ('mansplaining').\\n4. Prejudiced Discussion:\\n4.1 Support for the mistreatment of individual women: Justifying, denying, or understating the mistreatment of a woman.\\n4.2 Support for systemic discrimination against women as a group: Justifying, denying, or understating discrimination against women as a group.\\n\"\n",
    "        },\n",
    "        \"few_shot_config\": {\n",
    "            \"toxicidade\": {\"examples\": [], \"output_prefix\": \"results_toxic\"},\n",
    "            \"misoginia\": {\"examples\": [], \"output_prefix\": \"results_mysogyny\"}\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "214c239e-65f4-42b0-91b5-bc715cb00dfa",
   "metadata": {
    "id": "214c239e-65f4-42b0-91b5-bc715cb00dfa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "teste1_pt_sem_conceito - Fold 1 (toxicidade): 1it [00:29, 29.42s/it]\n",
      "teste1_pt_sem_conceito - Fold 1 (misoginia): 1it [00:25, 25.38s/it]\n",
      "teste1_pt_sem_conceito - Fold 2 (toxicidade): 1it [00:26, 26.16s/it]\n",
      "teste1_pt_sem_conceito - Fold 2 (misoginia): 1it [00:26, 26.81s/it]\n",
      "teste1_pt_sem_conceito - Fold 3 (toxicidade): 1it [00:26, 26.49s/it]\n",
      "teste1_pt_sem_conceito - Fold 3 (misoginia): 1it [00:26, 26.81s/it]\n",
      "teste1_pt_sem_conceito - Fold 4 (toxicidade): 1it [00:26, 26.80s/it]\n",
      "teste1_pt_sem_conceito - Fold 4 (misoginia): 1it [00:26, 26.85s/it]\n",
      "teste1_pt_sem_conceito - Fold 5 (toxicidade): 1it [00:27, 27.28s/it]\n",
      "teste1_pt_sem_conceito - Fold 5 (misoginia): 1it [00:27, 27.24s/it]\n",
      "teste2_en_sem_conceito - Fold 1 (toxicidade): 1it [00:25, 25.86s/it]\n",
      "teste2_en_sem_conceito - Fold 1 (misoginia): 1it [00:25, 25.89s/it]\n",
      "teste2_en_sem_conceito - Fold 2 (toxicidade): 1it [00:26, 26.01s/it]\n",
      "teste2_en_sem_conceito - Fold 2 (misoginia): 1it [00:26, 26.61s/it]\n",
      "teste2_en_sem_conceito - Fold 3 (toxicidade): 1it [00:25, 25.35s/it]\n",
      "teste2_en_sem_conceito - Fold 3 (misoginia): 1it [00:25, 25.73s/it]\n",
      "teste2_en_sem_conceito - Fold 4 (toxicidade): 1it [00:25, 25.64s/it]\n",
      "teste2_en_sem_conceito - Fold 4 (misoginia): 1it [00:25, 25.38s/it]\n",
      "teste2_en_sem_conceito - Fold 5 (toxicidade): 1it [00:25, 25.78s/it]\n",
      "teste2_en_sem_conceito - Fold 5 (misoginia): 1it [00:25, 25.78s/it]\n",
      "teste3_pt_com_conceito - Fold 1 (toxicidade): 1it [01:08, 68.11s/it]\n",
      "teste3_pt_com_conceito - Fold 1 (misoginia): 1it [01:01, 61.21s/it]\n",
      "teste3_pt_com_conceito - Fold 2 (toxicidade): 1it [01:07, 67.58s/it]\n",
      "teste3_pt_com_conceito - Fold 2 (misoginia): 1it [01:01, 61.59s/it]\n",
      "teste3_pt_com_conceito - Fold 3 (toxicidade): 1it [01:07, 67.24s/it]\n",
      "teste3_pt_com_conceito - Fold 3 (misoginia): 1it [01:01, 61.01s/it]\n",
      "teste3_pt_com_conceito - Fold 4 (toxicidade): 1it [01:07, 67.44s/it]\n",
      "teste3_pt_com_conceito - Fold 4 (misoginia): 1it [01:00, 60.82s/it]\n",
      "teste3_pt_com_conceito - Fold 5 (toxicidade): 1it [01:07, 67.70s/it]\n",
      "teste3_pt_com_conceito - Fold 5 (misoginia): 1it [01:01, 61.24s/it]\n",
      "teste4_en_com_conceito - Fold 1 (toxicidade): 1it [00:53, 53.30s/it]\n",
      "teste4_en_com_conceito - Fold 1 (misoginia): 1it [00:49, 49.18s/it]\n",
      "teste4_en_com_conceito - Fold 2 (toxicidade): 1it [00:53, 53.20s/it]\n",
      "teste4_en_com_conceito - Fold 2 (misoginia): 1it [00:49, 50.00s/it]\n",
      "teste4_en_com_conceito - Fold 3 (toxicidade): 1it [00:53, 53.16s/it]\n",
      "teste4_en_com_conceito - Fold 3 (misoginia): 1it [00:49, 49.22s/it]\n",
      "teste4_en_com_conceito - Fold 4 (toxicidade): 1it [00:53, 53.25s/it]\n",
      "teste4_en_com_conceito - Fold 4 (misoginia): 1it [00:48, 48.94s/it]\n",
      "teste4_en_com_conceito - Fold 5 (toxicidade): 1it [00:53, 53.58s/it]\n",
      "teste4_en_com_conceito - Fold 5 (misoginia): 1it [00:49, 49.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Para rodar para todos os zero shot de 1 só vez\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "# --- Configurações Iniciais ---\n",
    "CHUNK_SIZE = 404\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filename='processamento_llm.log', filemode='w')\n",
    "\n",
    "\n",
    "# --- LÓGICA DE PROCESSAMENTO UNIFICADA ---\n",
    "\n",
    "# Mapeamento para lidar com prompts em inglês, mantendo os nomes dos arquivos\n",
    "\n",
    "for test_config in all_tests:\n",
    "    test_name = test_config[\"name\"]\n",
    "    SYSTEM_PROMPT_BASE = test_config[\"system_prompt_base\"]\n",
    "    TASK_DEFINITIONS = test_config[\"task_definitions\"]\n",
    "    FEW_SHOT_CONFIG = test_config[\"few_shot_config\"]\n",
    "\n",
    "    logging.info(f\"############################################################\")\n",
    "    logging.info(f\"##### INICIANDO GRUPO DE TESTE: {test_name} #####\")\n",
    "    logging.info(f\"############################################################\")\n",
    "\n",
    "    for fold_number in range(1, 6):\n",
    "        for task_name_pt, config in FEW_SHOT_CONFIG.items():\n",
    "\n",
    "            logging.info(f\"============================================================\")\n",
    "            logging.info(f\"PROCESSANDO: Teste '{test_name}' - Fold {fold_number} - Tarefa: {task_name_pt}\")\n",
    "            logging.info(f\"============================================================\")\n",
    "\n",
    "            task_definition = TASK_DEFINITIONS.get(task_name_pt, \"\")\n",
    "            dynamic_system_prompt = SYSTEM_PROMPT_BASE.format(task_definition=task_definition)\n",
    "\n",
    "            sufixo_arquivo = \"\"\n",
    "            if task_name_pt == \"toxicidade\":\n",
    "                sufixo_arquivo = \"_TOXICIDADE\"\n",
    "            elif task_name_pt == \"misoginia\":\n",
    "                sufixo_arquivo = \"_MISOGINIA\"\n",
    "\n",
    "            input_filename = f\"folds_sem_hashtag/videos_fold_{fold_number}_estratificado{sufixo_arquivo}_LIMPO_SEM_HASHTAGS.csv\"\n",
    "\n",
    "            if not os.path.exists(input_filename):\n",
    "                logging.warning(f\"Arquivo {input_filename} não encontrado. Pulando.\")\n",
    "                continue\n",
    "\n",
    "            all_results_for_run = []\n",
    "            reader = pd.read_csv(input_filename, chunksize=CHUNK_SIZE)\n",
    "\n",
    "            for chunk_df in tqdm(reader, desc=f\"{test_name} - Fold {fold_number} ({task_name_pt})\"):\n",
    "                if chunk_df.empty:\n",
    "                    continue\n",
    "\n",
    "                batch_messages_text = chunk_df['text'].astype(str).tolist()\n",
    "                batch_conversations = []\n",
    "\n",
    "                prefix = \"Text:\" if 'en' in test_name else \"Texto:\"\n",
    "\n",
    "                for msg_text in batch_messages_text:\n",
    "\n",
    "                    formatted_msg = f\"{prefix} '{msg_text}'\"\n",
    "\n",
    "                    conversation = [{\"role\": \"system\", \"content\": dynamic_system_prompt}]\n",
    "                    conversation.extend(config.get(\"examples\", []))\n",
    "                    conversation.append({\"role\": \"user\", \"content\": formatted_msg})\n",
    "                    batch_conversations.append(conversation)\n",
    "\n",
    "                if not batch_conversations:\n",
    "                    continue\n",
    "\n",
    "                # O código do pipeline processa os prompts\n",
    "                batch_prompts_formatted = tokenizer.apply_chat_template(\n",
    "                    batch_conversations, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                batch_results = pipe(batch_prompts_formatted, **generation_params)\n",
    "\n",
    "                results_to_save_this_chunk = []\n",
    "                for i, result in enumerate(batch_results):\n",
    "                    llm_response = result[0]['generated_text'].strip()\n",
    "                    original_message = batch_messages_text[i]\n",
    "\n",
    "                    results_to_save_this_chunk.append({\n",
    "                        \"original_id\": chunk_df['id_video_anonimizado'].iloc[i],\n",
    "                        \"original_prompt\": original_message,\n",
    "                        \"llm_response\": llm_response\n",
    "                    })\n",
    "\n",
    "                if results_to_save_this_chunk:\n",
    "                    all_results_for_run.extend(results_to_save_this_chunk)\n",
    "\n",
    "            if all_results_for_run:\n",
    "                output_prefix = config[\"output_prefix\"]\n",
    "                # Formato do nome do arquivo de saída modificado para ser único\n",
    "                output_filename = f\"resultados_folds_sem_hashtag/{output_prefix}_{test_name}_fold_{fold_number}.csv\"\n",
    "\n",
    "                final_df = pd.DataFrame(all_results_for_run)\n",
    "                final_df.to_csv(output_filename, index=False)\n",
    "                logging.info(f\"Resultados salvos em: {output_filename}\")\n",
    "            else:\n",
    "                logging.warning(f\"Nenhum resultado gerado para Teste '{test_name}', Fold {fold_number}, Tarefa: {task_name_pt}\")\n",
    "\n",
    "logging.info(\"============================================================\")\n",
    "logging.info(\"TODOS OS TESTES FORAM CONCLUÍDOS!\")\n",
    "logging.info(\"============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8aeb1f9-d8b0-4932-a4f8-6fae7faf8679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oi'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'oi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7889f2-b198-4f9c-a1e6-fcb9302a2cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
