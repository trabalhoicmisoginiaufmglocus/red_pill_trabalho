{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673e69ea-581e-40d5-b497-0b6a8617c77e",
   "metadata": {
    "id": "673e69ea-581e-40d5-b497-0b6a8617c77e"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27765e-167c-4089-9daf-401780f3bc55",
   "metadata": {
    "id": "3a27765e-167c-4089-9daf-401780f3bc55"
   },
   "outputs": [],
   "source": [
    "# Separe aqui seus dados pra fazer a inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5e72e-2e37-436f-8169-f3d603684203",
   "metadata": {
    "id": "e5d5e72e-2e37-436f-8169-f3d603684203"
   },
   "source": [
    "# Setup Llama 3.1:8b Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe8a7c-8fd6-4bc7-8aa9-409756fdf2b5",
   "metadata": {
    "id": "16fe8a7c-8fd6-4bc7-8aa9-409756fdf2b5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7acdd5c-ade8-4253-b5e3-086d4c3c9a91",
   "metadata": {
    "id": "d7acdd5c-ade8-4253-b5e3-086d4c3c9a91"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='llm_inferences.log',\n",
    "    filemode='a'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264893b-8d1b-40cf-a217-286f73c29d49",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "004ccc75c8ce45518a2b0c3f8199fdf4"
     ]
    },
    "id": "7264893b-8d1b-40cf-a217-286f73c29d49",
    "outputId": "36a06432-96a7-413e-bf8c-6a0eb92217ed"
   },
   "outputs": [],
   "source": [
    "local_path = \"/home/jovyan/datalake/models/llama3-8b\"\n",
    "\n",
    "print(f\"Loading tokenizer from {local_path}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    local_path,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"pad_token defined as eos_token for tokenizer.\")\n",
    "\n",
    "print(f\"Loading model from {local_path}...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_path,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2626f56-2ac9-47be-948e-c885b7c37131",
   "metadata": {
    "id": "c2626f56-2ac9-47be-948e-c885b7c37131"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702294ef-90ed-4ab5-9bac-bba98793f243",
   "metadata": {
    "id": "702294ef-90ed-4ab5-9bac-bba98793f243"
   },
   "outputs": [],
   "source": [
    "generation_params = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"do_sample\": False,\n",
    "    #\"temperature\": 0.7,\n",
    "    \"truncation\": False,\n",
    "    \"return_full_text\": False,\n",
    "}\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410239a6-5ec1-4639-9549-53399ce2db29",
   "metadata": {
    "id": "410239a6-5ec1-4639-9549-53399ce2db29"
   },
   "outputs": [],
   "source": [
    "def save_batch_to_csv(batch, results_to_save):\n",
    "    \"\"\"\n",
    "    Save a batch result into a CSV file, creating the header if the file\n",
    "    does not exist or is empty.\n",
    "\n",
    "    Args:\n",
    "        batch (int): Batch ID\n",
    "        results_to_save (list): List of llm results obtained (prompt + output)\n",
    "    \"\"\"\n",
    "\n",
    "    output_csv_file = f\"llm_inferences/llama3.1-8b-instruct/{batch}.csv\"\n",
    "    fieldnames = [\"original_id\", \"original_prompt\", \"llm_response\"]\n",
    "\n",
    "    output_dir = Path(output_csv_file).parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    write_header = not os.path.exists(output_csv_file) or os.path.getsize(output_csv_file) == 0\n",
    "\n",
    "    try:\n",
    "        with open(output_csv_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "                logging.info(f\"Header written to '{output_csv_file}'.\")\n",
    "\n",
    "            writer.writerows(results_to_save)\n",
    "\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Error on saving batch to CSV '{output_csv_file}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fe421-7188-4515-91cc-b7f9ca0af43a",
   "metadata": {
    "id": "b76fe421-7188-4515-91cc-b7f9ca0af43a"
   },
   "outputs": [],
   "source": [
    "PROGRESS_FILE = 'inferences.json'\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load the progress from a JSON file.\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            logging.info(f\"Progress file '{PROGRESS_FILE}' found. Loading state.\")\n",
    "            return json.load(f)\n",
    "    logging.info(f\"No progress file '{PROGRESS_FILE}' found. Starting from zero.\")\n",
    "    return {}\n",
    "\n",
    "def save_progress(progress_data):\n",
    "    \"\"\"Save the current progress into a JSON file\"\"\"\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump(progress_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e7108-f1fb-445d-89eb-a022110ec9b6",
   "metadata": {
    "id": "bc7e7108-f1fb-445d-89eb-a022110ec9b6"
   },
   "outputs": [],
   "source": [
    "progress = load_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434c9b9-4b75-4dd0-bbd6-a37a29e0a333",
   "metadata": {
    "id": "a434c9b9-4b75-4dd0-bbd6-a37a29e0a333",
    "outputId": "8fbe9e62-3ea8-479f-de03-09b32b332a52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (toxicidade): 1it [01:57, 117.08s/it]\n",
      "Fold 1 (misoginia): 1it [01:27, 87.60s/it]\n",
      "Fold 2 (toxicidade): 1it [02:01, 121.69s/it]\n",
      "Fold 2 (misoginia): 1it [01:27, 88.00s/it]\n",
      "Fold 3 (toxicidade): 1it [02:02, 122.91s/it]\n",
      "Fold 3 (misoginia): 1it [01:28, 88.31s/it]\n",
      "Fold 4 (toxicidade): 1it [02:00, 120.56s/it]\n",
      "Fold 4 (misoginia): 1it [01:27, 87.77s/it]\n",
      "Fold 5 (toxicidade): 1it [02:00, 120.84s/it]\n",
      "Fold 5 (misoginia): 1it [01:28, 88.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "# --- Configura√ß√µes Iniciais ---\n",
    "CHUNK_SIZE = 404\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 1. System Prompt Gen√©rico e Direto\n",
    "# A idea eh s√≥ declarar uma persona pro llama incorporar, bem como a tarefa a ser realizada\n",
    "SYSTEM_PROMPT = (\n",
    "    \"Voc√™ √© um classificador de texto altamente preciso de coment√°rios do Reddit e Youtube. \" #S√£o os dois?\n",
    "    \"Sua √∫nica fun√ß√£o √© analisar a mensagem do usu√°rio e responder estritamente com '0' ou '1'.\\n\"\n",
    "    \"1: O texto pertence √† categoria solicitada.\\n\"\n",
    "    \"0: O texto N√ÉO pertence √† categoria solicitada.\\n\"\n",
    "    \"N√£o forne√ßa nenhuma explica√ß√£o, apenas o n√∫mero.\"\n",
    ")\n",
    "\n",
    "# 2. Exemplos de Few-Shot Estruturados como Di√°logo\n",
    "# Agr estamos meio que simulando uma conversa com exemplos pro modelo, ao inves de passar tudo como um contexto so\n",
    "FEW_SHOT_CONFIG = {\n",
    "    \"toxicidade\": {\n",
    "        \"examples\": [\n",
    "            {\"role\": \"user\", \"content\": \"‚Äò@ t√° vendo que relato padrinho, que essas pilantras s√£o capazes de aprontar, quem n√£o ouvir seus conselhos vai pra sarjeta, enquanto a pilantras vai t√° sentando em v√°rias mandiocas‚Äô\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"1\"},\n",
    "            {\"role\": \"user\", \"content\": \"‚ÄòEsse pai tem meu respeito. üëèüèªüëèüèªüëèüèªüëèüèª‚Äô\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"0\"},\n",
    "            {\"role\": \"user\", \"content\": \"‚ÄòImagina aguentar essa doida berrando no teu ouvido todo santo dia.‚Äô\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"1\"},\n",
    "            {\"role\": \"user\", \"content\": \"‚ÄòVERDADE CRIA JUIZO PAIZ√ÉO, METE MARCHA‚Äô\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"0\"},\n",
    "            {\"role\": \"user\", \"content\": \"‚Äòmeta mesmo gostoso mas n√£o use drogas e n√£o engravide usa camisinha e muito anticoncepcional meta mesmo vai tudo pro inferno seja feliz na terra‚Äò\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"1\"},\n",
    "        ],\n",
    "        \"output_prefix\": \"results_toxic\"\n",
    "    },\n",
    "    \"misoginia\": {\n",
    "        \"examples\": [\n",
    "            {\"role\": \"user\", \"content\": \"‚Äò@ t√° vendo que relato padrinho, que essas pilantras s√£o capazes de aprontar, quem n√£o ouvir seus conselhos vai pra sarjeta, enquanto a pilantras vai t√° sentando em v√°rias mandiocas‚Äô\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"1\"},\n",
    "            {\"role\": \"user\", \"content\": \"‚ÄòA gente precisa mesmo e de Deus‚Äô\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"0\"},\n",
    "            {\"role\": \"user\", \"content\": \"‚ÄòN√£o precisa ser raparigueiro para cair nesse golpe, basta ter o azar de dar carona para uma vagabunda destas da√≠, aqui em Bras√≠lia muitos coroas ca√≠ram nesse golpe de dar carona para certas minas bonitas!‚Äô\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"1\"},\n",
    "            {\"role\": \"user\", \"content\": \"‚Äòvaleu Copini, fiz varias  anota√ß√µes para aplicar em um futuro pr√≥ximo obrigado pelas dicas.‚Äô\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"0\"},\n",
    "            {\"role\": \"user\", \"content\": \"‚Äòcara, a mulherada de 17-18 aqui na escola t√° tudo virada na pomba gira!üòÇ Elas t√£o distribuindo..vc nem chega elas que chegam...precisa correr atr√°s de Tia Neide n√£o‚Äô\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"1\"},\n",
    "        ],\n",
    "        \"output_prefix\": \"results_mysogyny\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "for fold_number in range(1, 6):\n",
    "    for task_name, config in FEW_SHOT_CONFIG.items():\n",
    "\n",
    "        logging.info(f\"============================================================\")\n",
    "        logging.info(f\"INICIANDO PROCESSAMENTO: Fold {fold_number} - Tarefa: {task_name}\")\n",
    "        logging.info(f\"============================================================\")\n",
    "\n",
    "        # Define o nome do arquivo de entrada dinamicamente\n",
    "        sufixo_arquivo = \"\"\n",
    "        if task_name == \"toxicidade\":\n",
    "            sufixo_arquivo = \"_TOXICIDADE\"\n",
    "        elif task_name == \"misoginia\":\n",
    "            sufixo_arquivo = \"_MISOGINIA\"\n",
    "\n",
    "        input_filename = f\"comentarios_fold_{fold_number}_estratificado{sufixo_arquivo}.csv\"\n",
    "\n",
    "        if not os.path.exists(input_filename):\n",
    "            logging.warning(f\"Arquivo {input_filename} n√£o encontrado. Pulando para a pr√≥xima tarefa.\")\n",
    "            continue\n",
    "\n",
    "        all_results_for_run = []\n",
    "        reader = pd.read_csv(input_filename, chunksize=CHUNK_SIZE)\n",
    "\n",
    "        for chunk_index, chunk_df in enumerate(tqdm(reader, desc=f\"Fold {fold_number} ({task_name})\")):\n",
    "            if chunk_df.empty:\n",
    "                continue\n",
    "\n",
    "            batch_messages_text = chunk_df['comentario'].astype(str).tolist()\n",
    "            batch_conversations = []\n",
    "\n",
    "            # 3. Constru√ß√£o Din√¢mica da Conversa para Cada Mensagem\n",
    "            for msg_text in batch_messages_text:\n",
    "                # Inicia a conversa com o prompt de sistema\n",
    "                conversation = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "                \n",
    "                # Adiciona os exemplos de few-shot espec√≠ficos da tarefa\n",
    "                conversation.extend(config[\"examples\"])\n",
    "                \n",
    "                # Adiciona a mensagem real a ser classificada no final\n",
    "                conversation.append({\"role\": \"user\", \"content\": msg_text})\n",
    "                \n",
    "                batch_conversations.append(conversation)\n",
    "\n",
    "            if not batch_conversations:\n",
    "                continue\n",
    "\n",
    "            # O resto do c√≥digo funciona como antes, mas agora com o prompt conversacional\n",
    "            batch_prompts_formatted = tokenizer.apply_chat_template(\n",
    "                batch_conversations,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            batch_results = pipe(batch_prompts_formatted, **generation_params)\n",
    "\n",
    "            results_to_save_this_chunk = []\n",
    "            for i, result in enumerate(batch_results):\n",
    "                llm_response = result[0]['generated_text'].strip()\n",
    "                original_message = batch_messages_text[i]\n",
    "\n",
    "                result_to_save = {\n",
    "                    \"original_id\": chunk_df['id_comentario_anonimizado'].iloc[i],\n",
    "                    # Salva apenas a mensagem original, pois o prompt completo √© grande e repetitivo\n",
    "                    \"original_prompt\": original_message,\n",
    "                    \"llm_response\": llm_response\n",
    "                }\n",
    "                results_to_save_this_chunk.append(result_to_save)\n",
    "\n",
    "            if results_to_save_this_chunk:\n",
    "                all_results_for_run.extend(results_to_save_this_chunk)\n",
    "\n",
    "        # Ap√≥s processar todos os chunks, salva o resultado consolidado\n",
    "        if all_results_for_run:\n",
    "            output_prefix = config[\"output_prefix\"]\n",
    "            output_filename = f\"{output_prefix}_{fold_number}.csv\"\n",
    "\n",
    "            final_df = pd.DataFrame(all_results_for_run)\n",
    "            final_df.to_csv(output_filename, index=False)\n",
    "            logging.info(f\"Resultados consolidados salvos em: {output_filename}\")\n",
    "        else:\n",
    "            logging.warning(f\"Nenhum resultado gerado para Fold {fold_number} - Tarefa: {task_name}\")\n",
    "\n",
    "logging.info(\"============================================================\")\n",
    "logging.info(\"TODAS AS TAREFAS FORAM CONCLU√çDAS!\")\n",
    "logging.info(\"============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c239e-65f4-42b0-91b5-bc715cb00dfa",
   "metadata": {
    "id": "214c239e-65f4-42b0-91b5-bc715cb00dfa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
